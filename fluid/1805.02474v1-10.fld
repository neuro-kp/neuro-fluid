import scigen
import util
import dataset.scigen._1805_02474v1_10

def model_BiLSTM: model("BiLSTM", tableData)
def model_LSTM: model("LSTM", tableData)
def model_2_stacked_BiLSTM: model("2 stacked BiLSTM", tableData)
def model_3_stacked_BiLSTM: model("3 stacked BiLSTM", tableData)
def model_S_LSTM: model("S-LSTM", tableData)
def model_CNN: model("CNN", tableData)
def model_3_stacked_CNN: model("3 stacked CNN", tableData)
def mostEfficient:
   findWithKey_("time_s", minimum(map(lambda y: y.time_s, tableData)), tableData)

f"""As shown in Table 3, BiLSTM gives significantly
   {trendWord(model_BiLSTM.acc, model_LSTM.acc, betterWorse)}
   accuracies compared to uni-directional LSTM2, with the training time per epoch
   {trendWord(model_BiLSTM.time_s, model_LSTM.time_s, growShrink)} from
   {model_LSTM.time_s} seconds to {model_BiLSTM.time_s} seconds.
   Stacking 2 layers of BiLSTM gives {trendWord(model_2_stacked_BiLSTM.acc, model_BiLSTM.acc, improvements)}
   to development results, with a {trendWord(model_2_stacked_BiLSTM.time_s, model_BiLSTM.time_s, smallerHigher)}
   time of {model_2_stacked_BiLSTM.time_s} seconds. 3 layers of stacked BiLSTM
   {trendWord(model_3_stacked_BiLSTM.acc, model_BiLSTM.acc, improve)} the results.
   In contrast, S-LSTM gives a development result of {model_S_LSTM.acc}%, which is significantly
   {trendWord(model_S_LSTM.acc, model_2_stacked_BiLSTM.acc, betterWorse)}
   compared to 2-layer stacked BiLSTM, with a
   {trendWord(model_S_LSTM.param, model_2_stacked_BiLSTM.param, smallerHigher)}
   number of model parameters and a
   {trendWord(model_S_LSTM.time_s, model_2_stacked_BiLSTM.time_s, shorterLonger)}
   time of {model_S_LSTM.time_s} seconds. We additionally make comparisons with
   stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows),
   {mostEfficient.model} is the
   {rankLabel("most efficient", findIndex("model", "CNN", insertionSort(lambda a, b: a.time_s < b.time_s, tableData)))}
   among all models compared, with the
   {rankLabel("smallest", findIndex("model", "CNN", insertionSort(lambda a, b: a.param < b.param, tableData)))}
   model size. On the other hand, a 3-layer stacked CNN gives an accuracy of
   {numToStr(model_3_stacked_CNN.acc)}%, which is also the
   {rankLabel("lowest", findIndex("model", "CNN", insertionSort(lambda a, b: a.time_s < b.time_s, tableData)))}
   compared with BiLSTM, hierarchical attention and S-LSTM. The
   {rankLabel("best", findIndex("model", "S-LSTM+Attention", insertionSort(lambda a, b: b.acc < a.acc, tableData)))}
   performance of hierarchical attention is obtained by S-LSTM+Attention in terms of both accuracy and efficiency.
   S-LSTM gives significantly {trendWord(model_S_LSTM.acc, model_CNN.acc, betterWorse)}
   accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of
   BiLSTM and S-LSTM when external attention is used Attention leads to improved accuracies for both BiLSTM and
   S-LSTM in classification, with S-LSTM still
   {trendWord(model_S_LSTM.acc, model_BiLSTM.acc, underOverPerforming)} BiLSTM significantly."""
